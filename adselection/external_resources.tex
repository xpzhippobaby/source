\section{The Knowledgebase}
\label{sec:knowledge}
The emergence of large scale, probabilistic knowledgebases, such as
Probase~\cite{wu:manyconcepts} and Yago~\cite{SuchanekKW07yago},
brings new opportunities to text understanding.  We adopt Probase, a
probabilistic taxonomy for conceptualization in our approach.  The
reason we prefer Probase is that we consider a huge concept space
indispensable to understand open-domain bid phrases. Probase is rich
enough to cover a large proportion of concepts about worldly facts.
The version of Probase we use contains about 8.26 million instances
(e.g., ``windows phone 7'', ``ipad'', ``kindle'', etc.) and about 2.7
million concepts (e.g., ``platform'', ``device'', ``ebook reader'',
etc.).  In the rest of this paper, we use $e$ to denote an instance, $c$
as a concept.

All instances and concepts are organized hierarchically by the
\emph{isA} relationships.  For example, ``kindle'' is an instance of
concept ``ebook reader''.  Note that \emph{isA} relationships also
exist between sub-concepts and concepts (e.g., sub-concept ``ebook
reader'' is an instance of concept ``device'').  In contrast to
traditional taxonomies that regard knowledge as black and white,
Probase maintains probabilistic \emph{isA} relationships:
\begin{eqnarray}
%\begin{aligned}[b]
\Pr(e\vert c)=\frac{n(e,c)}{n(c)}\>\>\>\>\>\> \Pr(c\vert e)=\frac{n(e,c)}{n(e)}
%\Pr(c\vert e)=\frac{n(e,c)}{n(e)}
%\end{aligned}
\end{eqnarray}
where $n(c)$, $n(e)$, $n(e,c)$ denote the frequency of $c$, the
frequency of $e$, the frequency of $e$ and $c$ occuring together in all
Hearst patterns~\cite{Hearst92} extracted from billions of documents
and web pages.  These probabilities have some natural properties:
$P(e\vert c)$ reflects the typicality of $e$ given
$c$, and $P(c\vert e)$ reflects the typicality of
$c$ given $e$.  We use these probabilities as fundamental building blocks
for probabilistic models to conceptualize short texts.



We also use Probase's instance-instance co-occurrence network wherein each
node is a Probase instance and each edge between two nodes is the
co-occurrence relationship between instances.  For 8.26 million
instances, we extract their co-occurrence relationships from 1.68
billions of web pages.  Each time two instances co-occur within one
sentence of those web pages, we will increase their co-occurrence
frequency by 1.  Finally, we got a weighted network with 28.6 billion
edges.  After some filtering (such as removing edges with frequency
$\leq 5$), the ultimate network contains about 3.73 billions of edges.
We will use this co-occurrence information to disambiguate multiple
concepts of an instance to avoid enriching instances with noisy concepts.
%
%
%
%Clicks define a strong associations between queries and URLs.
%The semantics of a query is captured by its clicked
%URLs~\cite{fuxman:keywordgeneration}.
%Since our setting is sponsored search, we also exploit such valuable
%external knowledge.
%Our click-through data is collected through a commercial search engine
%and a mainstream web brower during one month period.
%Although the click-trhough data records various information, we are
%only interested in clicked URLs and search phrases.
%Thus, in the rest of this paper, we denote click-through data as tuples $Click Data=\{(q,u)\vert
%q\in Q,u\in U\}$ where $Q$ denotes all the search phrases and $U$
%denotes all the URLs.
%There are around 1.5 billions of such kind of tuples in our
%click-through data.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "adselection"
%%% End:
