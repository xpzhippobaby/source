\section{Conceptualization}
Conceptualization generalizes input to concepts. It is one of the most
basic cognitive process for human beings.  For instance, we generalize
``hot dog'' to the concept of ``food'', and ``dog'' to the concept of
``animal''.
%To do this, we need knowledge.  Probase provides rich
%conceptual knowledge that covers most worldly concepts, and allows
%machines to handle text understanding tasks as humans do.
To allow machines to handle such text understanding tasks, we exploit
rich conceptual knowledge provided by Probase that covers most worldly
facts.
%We divide the process of conceptualizing short text into three phases:
%(1)identifying entities from the short text;
%(2)inferring topics based on observed entities;
%(3)word sense disambiguation(WSD) by mining frequent
%concept-patterns.
\subsection{Instances Detection}
%By tokenization, each short text is regared as a sequence of tokens
%$T=\{t_i,i\in1,\ldots,L\}$.
%Say that an entity $e$ covers $T[i,j](j\geq i)$ if the consecutive
%subsequence of $T$ from $t_i$ to $t_j$ is identical to $e$.
%We detect entities from content of short text according to the
%following rule:
%\begin{balabala}
%$\forall i\in 1,\ldots,L$,identify $e$ which covers $T[i,j](j\geq
%        i)$ s.t. $\forall e^{'}$ that covers
%$T[i^{'},j^{'}](j^{'}\geq i^{'}),i^{'}\leq i\to j^{'}<j$
%\end{balabala}
%We consider each position in the token list as beginning position of
%eneity expecting to extract as many appropriate entities as possible
%so that they may provide richer hints of the semantics behind observed
%short text.
Given a short text such as ``windows phone app'', %we detect instances within the short text using a
%naive approach.
We firstly identify all Probase instances that appear
in the short text---``windows'', ``windows phone'', ``phone app'' and
``app''.  Next, we remove an instance if it is entirely
covered by another one (that is, it is a substring of another
instance). Thus after this step, our example short text is transformed
into a set of Probase instances $\{\text{``windows
    phone''},\text{``phone app''}\}$. 
%The reason we prefer instances of longest lengths lies in the
%fact % that compond words of open
% form % \footnote{there is space between
%   % each word of which the compound one consists} 
% are ubiquitous in
% English.  
%in most cases, the meaning of a compound word is a specialization of
%the head word.  In other words, 
In fact, an instance entirely covered by another
instance is very likely to be either its modifier or its head that has a
more general meaning.  Because conceptualization ``lifts'' the
representation of short text from term-level to concept-level, more
specific instances are better for avoiding ``over-abstracting''.%semantic drift.
%For
%example, within the short text ``windows phone app'', there are
%Probase instances $\textnormal{``windows''}_1$, $\textnormal{``windows
%  phone''}_1$, $\textnormal{``phone app''}_2$ and
%$\textnormal{``app''}_3$.  Note the subscript index indicates the
%beginning position of each instance.  We select ``windows phone''
%instead of ``windows'' becuase it is the longest cover beginging at
%position $1$.  Since ``app'' is entirely included by ``phone app'', we
%filter it even though it is the longest cover of position $3$.
%\end{example}
\subsection{Senses Derivation}
%The second step is to derive senses from observed entities.
%In our approach, each sense is characterized by a cluster of related
%concepts.
%In Probase, each entity is associated with a number of concepts that
%are the hypernyms of this entity.
%However, there are considerable number of ambiguous entities whose
%concepts should be partitioned into several clusters where each
%cluster expresses one sense of it.
%In Probase, Each instance $e$ maps to a set of concepts
%$\{c\vert\Pr(c\vert e)>0\}$. For each instance, we use an offline
%process to cluster its concepts and each cluster of concepts is
%treated as one sense of the instance.
%For example, the term ``apple'' may associate with one cluster
%that contains concepts such as ``fruit'', ``juice'', and another
%cluster that contains concepts such as ``company'', ``brand'', etc.
Sense of a word or phrase is its meaning.  To represent one sense of
an instance, we use a set of related concepts.  
See Table \ref{tab:re} for example, the set of all ``ipad'''s concepts $\{c\vert\Pr(c\vert
        \text{``ipad''})>0\}$ represents its only sense.
However, ambiguous instances such as ``apple'' has more than one sense and
associates totally unrelated concepts (e.g., ``fruit'' and ``company'')
in Probase.  We use an offline process to partition each instance's
concepts into one or more clusters where each cluster consists of
related concepts and thus represents one sense of the instance.
%Firstly, for each detected entity, we rank concepts of Probase by
%their likelihood given the entity(i.e., $\Pr(c\vert e)$) and reserve
%the top-$K$ concepts.
%Thus, each observed entity is associated with a concept vector
%$\mathbf{c}=(c_1,c_2,\ldots,c_{\vert C\vert})$.
%Note that normalization is needed because reserving only top-$K$
%concepts means truncating the entity's original distribution over
%concepts.
%Because number $K$ is usually of tens order, the concept vector
%$\mathbf{c}$ is extremely sparse compared with the dimensionality of Probase's concept space.
%However, transforming a short text containing only two or three terms
%into tens of explicit concepts is really an enrichment.
%Secondly, we split concept vectors of ambiguous entities.
%Actually, we cluster concepts within a concept vector $\mathbf{c}$
%based on the \textit{isA} relationships within them and the similarity
%between their entities as shown in Algorithm \ref{alg:clustering}:
% For clustering, how to decide initial clusters and how to determine
% the number of clusters are critical problems.  


%We devise a clustering algorithm (see Algorithm~\ref{alg:clustering})
For each instance $e$, we cluster its associated concepts by exploiting
\emph{isA} relationships and typicality.
% The most typical concepts of an entity
% are general, popular and thus representative for
% senses(clusters).%to which they belong.
% %Besides, the \emph{isA} relationships provided by Probase are of high quality
% %and more reliable than any statistical information.
% Our concept clustring algorithm is 
Given an instance $e$, we first rank its concepts by typicality (i.e.,
$\Pr(c\vert e)$) and select the top-$K$ most typical concepts for $e$.
The value of $K$ is usually in the order of tens (in our
        implementation $K=15$).  Then, we cluster the $K$ concepts
agglomeratively.  At the beginning, each of the $K$ concepts is
regarded as an independent cluster.  During the procedure, if there
exists isA relationship between any two concepts, their clusters are
merged.


Indeed, mainstream taxonomies (either automatically generated or
        manually annotated) are rather clean but not complete enough. 
For example, there is no isA relationship between the concept
``conventional input device'' and any other concepts in the first
sense (cluster) of ''mouse'' (see Table \ref{tab:iniclusters}).
%and thus it can't
%be absorbed into the first cluster of ``mouse'' due to the imcomplete
%isA relationships of Probase.
However, we can eventually merge them correctly by a simple rule: if a
certain concept is the suffix of another concept, merge their clusters
together. Actually, ``device'' is the head of not only ``conventional
input device'' but also most concepts in the first sense of ``mouse''.
%identify its correct sense
%by the fact that it share a common 
%One suffix of ``conventional input device'' is ``device'' and with many
%concepts of that sense and ``device'' itself is actually the head of
%most concepts of that sense.
%Up to now, if a concept still forms a cluster alone, we merge it with the cluster
%whose concepts has the largest number of intersecting hypernyms with it among
%all the clusters.
%\\in reality, I add the above heuristic rule for concept clustering,
    %however, we'd better eliminate it in our paper.


This procedure results in several initial clusters. Table
\ref{tab:iniclusters} shows initial clusters of some ambiguous
instances.  Let the instances of a cluster be the union of its
concepts' instances, we can assign other concepts associated with $e$
%(i.e.,
%$\{c\in C\vert\Pr(c\vert e)>0\}$) 
    to its closest cluster where
``closeness'' is measured by the proportion of overlapping instances
of them. % is represented by its
% distribution over all Probase's entities and each cluster is
% reresented by the mean of its initial concepts' representations.
% Cosine distance is adopted for distance measurement.
%\IncMargin{1em}
%\begin{algorithm}[!tb]
%\SetKwInOut{Input}{input}
%\SetKwInOut{Output}{output}
%\caption{CONCEPT CLUSTERING}
%\Input{instance $e$ and parameter $K$}
%\Output{Senses $\{s_1,\ldots,s_L\}$}
%%\BlankLine
%\Begin{
%Rank concepts by $\Pr(c\vert e)$\\
%\For {each concept $c_i$ where $i\leq K$} {
%    Initialize $c_i$ as a cluster $s_i$\;
%}
%\For {each concept pair $c_i$ and $c_j$ where $i,j\leq K$}{
%    \If {$c_i$ is an entity of $c_j$}{
%        merge clusters of $c_i$ and $c_j$\;
%    u
%}
%%\For {each concept $c_i$ where $i\leq K$}{
%%    \If {$c_i$ forms a cluster alone}{
%%        $CA\leftarrow\varnothing$\;
%%        $k\leftarrow i$\;
%%        \For {each concept $c_j\neq c_i$ where $j<K$}{
%%            $CA_{j}\leftarrow$ common concepts of $c_i$ and $c_j$\;
%%            \If {$\vert CA_{j}\vert>\vert CA\vert$}{
%%                $CA\leftarrow CA_j$\;
%%                $k\leftarrow j$\;
%%            }
%%        }
%%        \If {$k\neq i$}{
%%            merge clusters of $c_i$ and $c_k$\;
%%        }
%%    }
%%}
%\For {each cluster $s_i$}{
%    $\mathbf{dist_{s_i}}\leftarrow\mathbf{0}$\;
%    \For {each concept $c_{j}\in s_i$}{
%        $\mathbf{dist_{s_i}}+=$ distribution of $c_j$ over entities\;
%    }
%    $\mathbf{dist_{s_i}}\leftarrow\frac{\mathbf{dist_{s_i}}}{\vert
%        s_{i}\vert}$\;
%}
%\For {each concept $c_i$ where $i>K$}{
%    $\mathbf{dist_i}\leftarrow$ distribution of $c_i$ over entities\;
%    assign $c_i$ to the closest cluster;
%}
%\Return $\{s_1,\ldots,s_L\}$
%}
%\label{alg:clustering}
%\end{algorithm}
\begin{table*}[!htb]
\centering
\begin{tabular}{|c|c|c|}
\hline
Instance&Sense&Representative instances\\
\hline
\multirow{3}{0.5in}{$e_1=$ipad} &
\multirow{3}{3in}{$s_{1}=\{$tablet device, iso device, apple
device, platform, device, mobile device, portable device, technology,
    tablet device, tablet, gadget,
    $\ldots\}$} & \multirow{3}{3in}{$U_{1}=\{$iphone, mobile phone, ipod, laptop,
        ipod touch, pdas, smartphones, apple tv, apple's ipad, phone,
        notebooks, kindle, $\ldots\}$}\\& &\\& &\\
\hline
\multirow{9}{0.5in}{$e_2=$apple}&\multirow{3}{3in}{$s_{1}=\{$fruit tree, crop, tree,
    $\ldots\}$}&\multirow{3}{3in}{$U_{1}=\{$peach, mango, pear,
        cherry, banana, carrot, potato, pecan, sweet potato,
        $\ldots\}$}\\& &\\& &\\
   \cline{2-3}
        &\multirow{3}{3in}{$s_{2}=\{$fruit, food, fresh fruit, flavor, juice,
            snack, healthy snack,
            $\ldots\}$}&\multirow{3}{3in}{$U_{2}\{$grape,
                banana, orange, strawberry, pear, peach, mango,
                cheese, cherry, chocolate, $\ldots\}$}\\& &\\& &\\
   \cline{2-3}
        &\multirow{3}{3in}{$s_{3}=\{$company, brand, firm, corporation,
            $\ldots\}$}&\multirow{3}{3in}{$U_{3}=\{$microsoft, ibm, sony, dell,
                motorola, google, intel, hp, nokia, cisco,
                $\ldots\}$}\\& &\\& &\\
\hline
\end{tabular}
\caption{Conceptualization of short text ``ipad apple''}
\label{tab:re}
\end{table*}
\begin{table*}[!htb]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
apple&blackberry&palm&bass&mouse\\
\hline
fruit&fruit&plant&warm water fish species&cursor control\\
food&berry&tree&fish&device\\
fresh fruit&small fruit&plant&predator&input device\\
flavor&food&*****&fish species&peripheral usb device\\
%fruit juice&technology&company&landscaping&carnivore\\
juice&dark berry&oil&*****&peripheral device\\
snack&fresh fruit&vegetable oil&wood&conventional input device\\
healthy snack&plant&tropical oil&*****&external device\\
*****&dark fruit&brand&instrument&computer peripheral\\
company&*****&edible oil&musical instrument&computer input device\\
brand&device&plant oil&stringed instrument&*****\\
firm&mobile device&*****&acoustic instrument&mammal\\
corporation&smartphones&device&sound&animal\\
*****&smart phone&personal digital assistant&traditional instrument&small animal\\
fruit tree&platform&pdas&&mammalian cell\\
crop&phone&handheld computer&*****&model organism\\
tree&handheld device&platform&beer&rodent\\
\hline
\end{tabular}
\caption{Initial Clusters of Ambiguous Instances}
\label{tab:iniclusters}
\end{table*}
%\DecMargin{1em}
%Another critical question within classical clustring algorithms is how
%to determine the number of clusters.
%According to our algorithm, the number of senses $L$ is not
%pre-defined.
%Actually, it is inderectly determined by the threshold $\theta$ which
%specifies how similar two clusters to be merged together should be.



%According to the clustring results $\{s_1,\ldots,s_L\}$, 
%By our concept clustring algorithm, we derive senses
%$\{s_1,\ldots,s_L\}$ for each given entity $e$.

% For an entity $e$, let $\mathbf{c}$ denote its concept distribution.
% We assign concept vector $\mathbf{c^{(i)}}$ for each sense $s_i$ where
% the $j$th entry of $\mathbf{c^{(i)}}$ equals the $j$th entry of
% $\mathbf{c}$ if the $j$th entry's corresponding concept is an element
% of $s_i$, or zero otherwise.  Then we normalize each sense $s_i$'s
% corresponding concept vector $\mathbf{c^{(i)}}$ to be unit vector.  In
% our implementation, we derive senses of each Probase entity in
% advance.

%\begin{example}
%Given entity ``apple'', we derive concept vector
%(``fruit''\:0.25, ``food''\:0.25, ``company''\:0.2, ``IT
% company''\:0.1, ``brand''\:0.2).
%Since ``fruit'' is one kind of ``food'' and ``IT company'' is an
%instance of ``company'' in Probase, we derive three clusters based on
%\textit{isA} relationships within these concepts(i.e.,
%        $g_{1}=\{\textnormal{``fruit''},\textnormal{``food''}\}$,
%        $g_{2}=\{\textnormal{``company''},\textnormal{``IT company''}\}$
%        and $g_{3}=\{\textnormal{``brand''}\}$).
%Because many entities such as ``apple'', ``microsoft'', ``amazon'', etc
%are shared by both ``company'' and ``brand'', we merge the last two
%clusters and split original concept vector into two vectors with
%normalization: (``fruit''\:0.5, ``food''\:0.5), (``company''\:0.4,
%        ``IT company''\:0.4, ``brand''\:0.2).
%\end{example}
%The probabilities of concepts are estimated by a naive Bayes
%model\cite{song:probabilisticknowledgebase} and Laplace smoothing is
%adopted for introducing concept diversities:
%\begin{equation}
%\label{eqn:naivebayesian}
%p_k=P(c_k\vert E)=\frac{P(E\vert c_k)P(c_k)}{P(E)}\propto
%P(c_k)\prod_{i=1}^{M}P(e_i\vert c_k)
%\end{equation}
%Our model is under the assumption that given a certain concept, the
%appearrances of different entities are independent with each other.
%It seems not so reasonable.
%However, Probase only provides frequency of co-occurrences between
%concepts and entities to estimate $P(e\vert c)$ and $P(c\vert e)$
%while no information for us to model the mutual influence between
%different entities.
%The resulting concept set for short text ``ipad apple'' is illustrated
\subsection{Senses Disambiguation}
%The procedure of merging entities is listed in Algorithm \ref{alg:merging}.
%\IncMargin{1em}
%\begin{algorithm}
%\SetKwInOut{Input}{input}
%\SetKwInOut{Output}{output}
%\caption{MERGING ENTITIES}
%\Input{Observed entities $\{e_{1},\ldots,e_{M}\}$ and their related
%    concept vectors and threshold $\beta$}
%\Output{Entity clusters $\{h_{1},\ldots,h_{D}\}$}
%%\BlankLine
%\Begin{
%Make each entity $e_i$ an individual cluster $h_i$\\
%\For {each pair of clusters $h_i$ and $h_j$}{
%    \textit{MaxSim}$\leftarrow 0$\\
%    $\mathbf{c_{\textnormal{sim}}}\leftarrow\mathbf{0}$\\
%    \For {each pair of their concept vectors $\mathbf{c_{i}^{(k)}}$ and $\mathbf{c_{j}^{(l)}}$}{
%        \If
%        {$\cos\{\mathbf{c_{i}^{(k)}},\mathbf{c_{j}^{(l)}}\}>\textnormal{\textit{MaxSim}}$}{
%                                                                                              \textit{MaxSim}$\leftarrow\cos\{\mathbf{c_{i}^{(k)}},\mathbf{c_{j}^{(l)}}\}$\\
%    $\mathbf{c_{\textnormal{sim}}}\leftarrow\mathbf{c_{i}^{(k)}}+\mathbf{c_{j}^{(l)}}$
%        }
%    }
%    \If {\textit{MaxSim}$\geq\beta$}{
%        merge $h_i$ and $h_j$ into one entity cluster $h_i\cap h_j$\\
%        concept vector of merged entity cluster $\leftarrow$ normalize
%        $\mathbf{c_{\textnormal{sim}}}$
%    }
%}
%\Return $\{h_{1},\ldots,h_{D}\}$
%}
%\label{alg:merging}
%\end{algorithm}
%\DecMargin{1em}
%We disambiguate many senses of entities.
Suppose we detected instances $\{\text{``ipad'', ``apple''}\}$ from short text
``ipad apple'' and the clustering results of these two instances are
as Table \ref{tab:re} shows.
Our goal is to identify the ``company'' sense as ``apple'' refers to
in this short text.
%We have detected instances from a given short text, and each instance is
%associated with one or more senses.  Our goal is to determine, for
%each instance, which sense is the most appropriate one given the context
%of the short text.  
Formally, we define the task of senses disambiguation as follows:
\begin{definition}
  Given a set of instances
      $E=\{e_1,\ldots,e_{D}\}$ where each instance is associated
  with one or more senses, e.g., $e\in{}E$ has $l$ senses
  $s_{1},\ldots,s_{l}$.  We define a sense-pattern as a vector
  $\mathbf{p}=(p_{1},\ldots,p_{D})^{\text{\rm{T}}}$ where $p_i$
  indicates that $e_i$ refers to its $p_i$-th sense in this context.  Senses
  disambiguation is to find the correct sense-pattern.
  %A sense-pattern
  %$\mathbf{p}=(p_{1},\ldots,p_{D})$ is a vector that
  %%consists of 
  %indicates one sense for each instance.  There are totally
  %$\prod_{i=1}^{D}L_i$ patterns and we want to find the best pattern
  %$\mathbf{p^{*}}$ that indicates the appropriate sense for each $e_i$.
\end{definition}

Given an ambiguous instance such as ``apple'', without any context, it
is impossible to judge which sense of it is expressed.
%For example, given a
%single word ``apple'', it is not clear which sense it refers to.
However, if the instances detected from short text are ``apple'' and
``microsoft'', we know that it refers to ``company'' instead of
``fruit''.
%\end{example}
%For each pair of instances $e_i$ and $e_j$, we compare each pair of
%their senses $(s_{k}^{(i)},s_{l}^{(j)})$. %  where $k=1,\ldots,L_i$
For each pair of instances $e$ and $e'$, suppose $e$ has senses
$s_{1},\ldots,s_{m}$ and $e'$ has senses
$s_{1}',\ldots,s_{n}'$, we compare each pair of their senses.
% and $l=1,\ldots,L_j$).  
%We measure the similarity between the two senses by computing the
%cosine similarity of their corresponding concept vectors
%$\mathbf{c_{k}^{(i)}}$ and $\mathbf{c_{l}^{(j)}}$ (that is, each component corresponds to one concept of Probase.
%If $c\in S_{k}^{(i)}$, the value of $c$'s corresponding component in
%$\mathbf{c_{k}^{(i)}}$ is $\Pr(c\vert e_{i})$ and zero otherwise).
%%(that is, each element of $\mathbf{c_{k}^{(i)}}$ represents a concept of sense
%%$\mathbf{s_{k}^{(i)}}$).
%Suppose $(s_{k^*}^{(i)},s_{l^*}^{(j)})$ has
%the maximum cosine similarity among all pairs of senses.  If
%$\cos(s_{k^*}^{(i)},s_{l^*}^{(j)})$ exceeds our pre-defined threshold,
%we reserve $s_{k^*}^{(i)}$, $s_{l^*}^{(j)}$ for $e_i$, $e_j$ and
%eliminate all other senses of the two instances.
We measure the similarity between $s_{i}$ and $s_{j}'$ by 
%the ratio
%of the number of overlapping concepts to the number of concepts in the
%smaller sense:
%$\frac{\vert s_{i}\cap{}s_{j}^{'}\vert}{\min(\vert s_{i}\vert,\vert
%        s_{j}^{'}\vert)}$.
Jaccard Similarity: JS$(s_{i},s_{j}')=\frac{\vert
    s_{i}\cap{}s_{j}'\vert}{\vert s_{i}\cup{}s_{j}'\vert}$.
%We measure the similarity between  $s_{k}^{(i)}$ and $s_{l}^{(j)}$ by
%the ratio of the number of overlapping concepts to the number of
%concepts in the smaller sense:
%$\frac{\vert
%    s_{k}^{(i)}\cap{}s_{l}^{(j)}\vert}{\min(\vert s_{k}^{(i)}\vert,\vert
%            s_{l}^{(j)}\vert)}$.
Suppose the pair $(s_{i},s_{j}')$ achieves the maximum Jaccard similarity among all pairs of
senses and the similarity exceeds our pre-defined threshold, we
reserve $s_{i}$, $s_{j}'$ for $e$, $e'$ respectively
and eliminate all other senses of the two instances.
%For each entity $e_i,i=1,\ldots,M$, we rank concepts by $P(c\vert
%        e_i)$ to derive its concept cluster $C_i=\{c_{i1},\ldots,c_{iK}\}$.
%Given a pair of adjacent entities $e_i, e_j$, if the intersection
%between their concept clusters are substantially large, the two
%entities are much likely to belong to the same category(e.g., both
%        ``apple'' and ``microsoft'' are IT companies).
%In fact, when $\vert C_i\cap C_j\vert\geq\frac{K}{2}$, we filter
%inappropriate concepts in $C$ out based on the following rule:
%\begin{balabala}
%$\forall c\in C$, filter $c$ s.t. $c\in C_i \cup C_j\land c\notin C_i\cap
%C_j\land(\forall k,k\neq i\land k\neq j\to c\notin C_k)$
%\end{balabala}



%In previous phase, we merge entities together and reserve their common
%topic so that we determined which topic the ambiguous entity cluster
%refers to in such context.
%Theoretically speaking, this strategy for WSD only capture
%\textit{isA} relationships provided by Probase.
However, two instances that share similar senses may not appear within
one short text at the same time.
%belong to the
%same concept and  do not show similarity.  
For instance, ``ipad apple'' %and call the
%relationship between ``ipad'' and ``apple'' as \emph{isProductOf}.
has three possible sense-patterns, ``device-tree'', ``device-fruit'' and
``device-company''.  Intuitively, ``company'' and ``device'' have
stronger \emph{contextual-continuity} than the other sense-patterns.  However, ``company'' and ``device'' are two totally different
senses, and the Jaccard similarity of them %may be very low.
is certainly below our pre-defined threshold.


%Patterns with strong contextual-continuity are popular(frequent) in
%human language while it is not the case for patterns with weak
%contextual-continuity.
%To solve this problem, we make an important observation.  
From our dataset, we observed that there are 
many pairs that fit the sense-pattern ``company-device'', and the instances 
in the pairs are not ambiguous, for example, ``amazon kindle'',
``microsoft surface'', ``google nexus'', etc.  Besides, ``amazon'',
``microsoft'' and ``google'' are all representative instances of the
sense ``company'' and ``kindle'', ``surface'' as well as ``nexus'' are
all representative instances of the sense ``device''. 
%Thus,
Based on the above observation, one immediate intuition is that two senses
have strong contextual-continuity their representative instances have
high co-occurrence.
%without any ambiguity(e.g., ``kindle amazon'', ``surface microsoft'',
%        etc.) are very popular(frequent).
%Patterns with strong contextual-continuity are much more
%popular(frequent) in human language than patterns with weak
%contextual-continuity.
%There are many short texts that contain entities also satisfying
%the so called \emph{isProductOf} relationship such as ``iphone
%apple'', ``kindle amazon'', ``surface microsoft'' and so on.
%Note that ``iphone'', ``kindle'', ``surface'' all belong to the
%``device'' sense and ``apple'', ``amazon'' and ``microsoft'' all belong
%to the ``company'' sense.
%%Although ``apple ipad'' has more than one possible
%%patterns---``fruit-device'', ``company-device'' and ``tree-device'', the
%``company-device'' pattern reflects better
%\emph{contextual-continuity} and is much more popular(i.e., frequent)
%    than the other two patterns.
%Based on above analysis, we approximate various kinds of relationships
%by \emph{contextual-continuity} and measure the
%Based on the above intuition,
Thus, we measure contextual-continuity between
two senses using our instance-instance co-occurrence network mentioned
in Section \ref{sec:knowledge}. 
Let us denote the network as a weighted
directed graph $G$ where each node denotes an instance and the weight
$w_{(u,v)}$ of edge $(u,v)$ denotes the co-occurrence
frequency of instances $u$ and $v$.  We also denote the occurrence
frequency of instance  $u$ by $w(u)=\sum_{v\in G}w(u,v)$ and the total
number of counted occurrences by $W=\sum_{u\in G}w(u)$.
%We estimate the probability of jumping from node $e$ to node
%$e^{'}$ by $\Pr(e^{'}\vert e)=\frac{w_{(e^{'},e)}}{\sum_{t}w_{(t,e)}}$. 
First, suppose an instance $e$ has senses $s_{1},\ldots,s_{m}$.  For
each sense $s_{i}, i\in\{1,\ldots,m\}$, we associate it with a set of
instances (denoted by $U_{i}$).  Elements of $U_{i}$ are top-K nodes
$u\in{}G$ when ranked by $\Pr(u \vert e, s_{i})$.  We compute the
probability with the following independence assumptions for
simplification:
\begin{equation}
\Pr(u\vert e, s_{i})\propto\Pr(u)\Pr(e, s_{i}\vert
        u)=\Pr(u)\Pr(e\vert u)\prod_{c\in s_{i}}\Pr(c\vert u)
\end{equation}
where $\Pr(u)$, $\Pr(c\vert u)$ are directly given by popularity and
    typicality and $\Pr(e\vert u)$ is estimated by
    $\frac{w(u,e)}{w(u)}$.  By normalization, we have $\Pr(u\vert
            e,s_{i})$ for each element $u\in U_{i}$.
%We only reserve the top-$K$ adjacent nodes and denote them  %sense $s_{k}^{(i)}$
%by $U_{k}^{(i)}$.  
The last column of Table \ref{tab:re} shows the top-ranking nodes of
the different senses of ``apple''.  As you can see, they are representative
instances of corresponding sense.  
Thus, given two instances $e$ and $e'$, having senses
$s_1,\ldots,s_{m}$ and $s_{1}',\ldots,s_{n}'$ respectively, we can measure the \emph{contextual-continuity (CC)} between
one pair of their senses $s_i$ and $s_{j}'$ by measuring the
connectivity between $U_{i}$ and $U_{j}'$.
First, we weight the connectivity between one pair of nodes $(u,v)$ by
\begin{definition}
\begin{equation}
\text{C}(u,v)=w(u,v)\log(\frac{W}{w(u)})\log(\frac{W}{w(v)})
\end{equation}
\end{definition} 
Our weighting technique is like the famouse TF-IDF where the co-occurrence frequency $w(u,v)$ is like the
    term frequency (TF) and the other terms function as inverse
    document frequency (IDF). 
%We define the \emph{contextual-continuity} between two
%senses as follows:
Then we compute the connectivity between $U_i$ and $U_{j}'$ based
on the connectivities between their nodes:
\begin{definition}
\begin{equation}
%\begin{aligned}
\textnormal{\emph{CC}}(s_{i},s_{j}')=\sum_{(u,v),u\in U_{i},v\in
    U_{j}'}\Pr(u\vert e,s_{i})\Pr(v\vert
            e',s_{j}')\text{C(u,v)}
%\end{aligned}
\end{equation}
\end{definition}
%Given two topics $g_i$, $g_j$ and their associated entity vector
%$\mathbf{e_i}$ and $\mathbf{e_j}$, we define their
%\textit{contextual-continuity} to be:
%\begin{definition}
%\begin{equation}
%\textnormal{\textit{contextual-continuity}}(g_{i},g_{j})=\sum\mathbf{e_i}^{(u)}\mathbf{e_j}^{(v)}w(u,v)
%\end{equation}
%\end{definition}
%where $w(u,v)$ is the weight of edge $(u,v)$ and $\mathbf{e_i}^{(k)}$
%denotes the $k$th entry of entity vector $\mathbf{e_i}$.
%Note that our entity vectors are extremely sparse and we can compute
%the \textit{contextual-continuity} between topics efficiently.


%\begin{table}
%\centering
%\begin{tabular}{|c|c|c|c|}\hline
%ipad&apple$^1$&apple$^2$&apple$^3$\\\hline
%iphone&peach&grape&microsoft\\
%mobile phone&mango&banana&ibm\\
%ipod&pear&orange&sony\\
%laptop&cherry&strawberry&dell\\
%ipod touch&banana&pear&motorola\\
%pdas&carrot&peach&google\\
%smartphones&potato&mango&intel\\
%apple tv&pecan&cheese&hp\\
%apple's ipad&sweet potato&cherry&nokia\\
%netbooks&magic mouse&chocolate&cisco\\\hline
%\end{tabular}
%%\caption{Representative Adjacent Nodes of ``ipad'' and ``apple''}
%\caption{Representative Instances}
%\label{tab:repent}
%\end{table}


We select the correct sense-pattern according to Eq.\eqref{eqn:p}
\begin{equation}
\label{eqn:p}
\mathbf{p}^{*}=\arg\max_{\mathbf{p}}\sum_{i=1}^{D}\max_{j\neq
    i}\textnormal{\emph{CC}}(s_{p_i},s_{p_{j}})
\end{equation}
%Then we construct the given short text's concept vector $\mathbf{c}$
%according to the most appropriate pattern $p^{*}$, where
%\begin{equation}
%\label{eqn:cv}
%\mathbf{c}=\frac{1}{D}\sum_{i=1}^{D}\frac{\mathbf{c_{p_{i}^{*}}^{(i)}}}{\vert\mathbf{c_{p_{i}^{*}}^{(i)}}\vert}
%\end{equation}
%$\mathbf{p}^{*}$ indicates the appropriate sense of each detected
%    instance.  
We disambiguate many senses of detected instances according to $\mathbf{p}^{*}$
and denote the chosen senses as $(s_1,\ldots,s_{D})$.  Then we construct the given short text's corresponding 
concept set as $\cup_{i=1}^{D}s_{i}\}$.
%Since each concept vector of a sense is a unit vector, the resulting
%concept vector $\mathbf{c}$, which is the mean of several unit
%vectors, is also a unit vector. Thus, no additional normalization is
%needed.
%By conceptualization, we finally map each short text into Probase's
%concept space and represent it as a unit concept vector.
By conceptualization, we eventually ``lift'' short text from bag-of-words to
bag-of-concepts.  Although enriching a short text into a set of
many concepts is a big step, it hasn't made full use of the
probabilistic taxonomy (i.e., Probase's typicalities).  
Thus, we also assign one concept vector $\mathbf{c}_{i}$ to sense $s_{i}$ where each
component of $\mathbf{c}_{i}$ corresponds to one concept in Probase and suppose the row $r$
corresponds to concept $c$, then the value in the row $r$ of
$\mathbf{c}_{i}$ is $\Pr(c\vert e_i)$ if
$c\in s_{i}$. Otherwise, it is zero.  Besides of the
concept set, we also assign the short text a concept vector:
$\sum_{i=1}^{D}\mathbf{c}_{i}$.
%Based on above analysis, we use our large scale entity-entity
%co-occurrence network as well as \textit{isA} relationships to
%approximate all kinds of other relationships.
%Let's denote it as a weighted undirected graph where each node
%$v_i$ denotes an entity $e_{v_i}$ and we do not make
%distinguish between entity $e_{v_i}$ and its corresponding node $v_i$
%from now on.
%The weight $w_{ij}$ of edge $(v_i,v_j)\in E$ is the frequence of
%co-occurrences of $v_i$ and $v_j$.
%
%
%
%Firstly, to do WSD for a pair of entities $v_i$ and $v_j$, we assign
%score to their common adjacent nodes.
%In our scheme, Common adjacent node $v_k$ is scored by:
%\begin{equation}
%\label{eqn:nodeweighting}
%s_k=w_{ik}\ast w_{jk}\ast \log
%\frac{\sum_{i^{'},j^{'}}w_{i^{'}j^{'}}}{\sum_{i^{'}}w_{i^{'}k}}
%\end{equation}
%Then we rank these common adjacent nodes by their assigned score in
%descending order and reserve top-$K$ entities $E^{'}$.
%Note that our scoring scheme is similar with TF-IDF where $w_{ik}\ast
%w_{jk}$ works as term frequence(TF) measuring the importance of $v_k$
%with respect to $v_i$ and $v_j$.
%The rest part of right-hand side of Eq \ref{nodeweighting} works as
%inverse document frequency(IDF) to measuring how common $v_k$ is
%across all entities.
%
%
%
%Secondly, we abstract a set of representative concepts $C^{'}$ according
%to $E^{'}$ in the same way as previous phase.
%If the the number of overlapping concepts between $C_i$ and $C^{'}$ is
%sufficient large(e.g., $\vert C_i \cap C^{'}\vert\geq\frac{K}{2}$), we
%use $C_i \cap C^{'}$ to filter inappropriate concepts of $C$ according
%to the following rule:
%\begin{balabala}
%$\forall c\in C$, filter $c$ s.t. $c\in C_i\land c\notin C_i\cap
%C^{'}\land(\forall k,k\neq i\to c\notin C_k)$
%\end{balabala}
%For each pair $e_i, e_j$, remember to repeat the filter procedure by
%replace $i$ by $j$ in the above rule.
%Some representative examples are illustrated in Table
%\ref{tab:conceptualization}.
%\begin{table}
%\centering
%\caption{Examples of conceptualization}
%\begin{tabular}{|c|c|c|}\hline
%``ipad apple''&``microsoft apple''&``ipad apple''\\
%without WSD&&with WSD\\\hline
%fruit&company&company\\
%company&brand&device\\
%food&corporation&mobile device\\
%device&firm&brand\\
%mobile device&client&platform\\
%portable device&large company&technology\\
%fresh fruit&...&...\\
%brand&&\\
%...&&\\
%\hline
%\end{tabular}
%\label{tab:conceptualization}
%\end{table}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "adselection"
%%% End:
