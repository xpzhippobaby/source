\section{Retrieval}
\label{sec:retrieval}
By conceptualization, we transform each short text (either a query or
        bid phrase) into a concept-level representation. Thus we can measure semantic similarity between short
texts over such representation and retrieve the most similar bid
keywords with respect to a query as its expanding results.


In practice, search engines offline expanded millions of historical
queries so that when a query is submitted, they immediately
know its relevant bid keywords if the query has appeared before (i.e.,
        in the preprocessing results). Otherwise, regular practice is to avoid advertising for this query.


To offline expand 30 million query $Q=\{q\}$ with a
keywords set consists of 0.7 billion bid keywords $P=\{p\}$, if we
simply compare each query $q$ with every bid keyword $b$, then the
number of comparisons is $O(\vert{}P\vert{}\vert{}Q\vert{})$. Even
though we make use of a map/reduce cluster, since the keywords set $P$ can not
fit into a node's main memory, the improvement is limited.  As for inverted
index that maps Probase concepts to bid keywords, it seems helpful but
the unbalanced data will drastically weaken its benefits.
Unfortunately, efficiency issue becomes critical when the data set is
of such a huge scale.  Besides, the expanding results should be
updated periodically. hence a preprocessing procedure which takes tens
of days will not be allowed.


%Besides of a collection of 0.7 billion bid phrases $P=\{p\}$ provided
%by a commercial search engine, we extract a collection of 30 million
%queries $Q=\{q\}$ from search logs. 
%We offline expand $Q$ using $P$ (i.e., for each $q\in Q$, select
%        several most related bid phrases from $P$).
%Then we build index mapping each $q$ to its resulting expansions. 
%When a query is issued by user, search engine look up the index,
%     if the given query is in that index, its expansions (related bid
%             phrases) are used to trigger ads for this query.
%We also build up an online query expansion system, so when the
%submitted query is unseen (not in $Q$), we retrieve related bid phrases for it
%through our online system. 
%%%%%%%%%%%%%%%%%%%%%%%%version sep%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%By conceptualization, we model both queries and bid phrases as concept
%vector and thus can measure similarity between short texts by
%measuring similarity between their corresponding concept vectors with
%conventional metrics.
%Like traditional IR system which builds inverted index mapping from
%words to documents, we can significantly improve the efficiency of
%retrieval by building inverted index mapping concepts to lists of bid
%phrases.
%However, in our setting, the number of bid phrases is on the order of
%a billion and some concepts contain extremely long inverted lists
%which incur significant overhead for building and accessing them.
%Moreover, according to our experiment, these popular concepts can't be
%treated as ``stop words'' and have to be reserved.
%Besides of offline expanding 30 million queries with 0.7 billion bid
%phrases for a commercial search engine, our aim is to construct a
%real-time query expansion system using billions of bid phrases.
%Thus the response time must be no more than tens of
%millisecond and the biggest challenge is efficiency.


We propose a two phase approach to retrieve relevant bid
keywords for a query.  Our approach is efficient enough for offline processing
massive data sets within acceptable period, or even applying to expand
unseen queries online.
First, we find a small set of candidate bid keywords (somehow relevant).  Second, we rank candidates by measuring their semantic similarity with the given query and return the top-$K$ bid keywords as expanding results.


On the one hand, we expect that the set of candidate bid keywords to be small enough so
that computational cost for ranking them is minimized.  On the other hand,
the candidates should cover adequate relevant bid keywords.  For
popular queries, our candidates selection method exploit their user
behavior data.  For tail ones, we build some semantic-aware index.
%Candidates selection requires that our approaches should make
%compromise between effectiveness(mainly recall) and efficiency.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%vertsion sep%%%%%%%%%%%%%%%%%%%%%%%%%%%
%On the one hand, we want the set of candidates to be small enough so
%that computational cost for ranking is minimized.  On the other hand,
%the candidates should cover adequate relevant bid phrases.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%version sep%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%not miss optimal bid phrases and recall
%as many optimal ones as possible.
% Since the first phase reduces the number of bid phrases for further
% considered, conputational effort for ranking can satisfy the
% efficiency requirements of online system.
%function can compare pairs of short texts in a
%meticulous way while satisfing the efficiency requirement of onine
%system.
%In practice, any query expansion approach must be efficient enough to
%process billions of queries within a tolerable period.
%Regular solution for such efficiency requirement is building inverted
%index.
%In our setting, We should build lookup table through which we can find
%related bid keywords of a given concept efficiently.
%By conceptualization, we can derive a set of concepts for given query.
%Then we retrieve relevant bid keywords of those concepts using our
%preprocessed lookup table.
%Actually, the number of concepts of Probase is much smaller than the
%number of bid keywords to be processed and the numbers of related bid
%keywords of some general concepts\footnote{These concepts shouldn't be
%    treated as stop words like term-matching approach does} are so
%    large.
%As an extreme case, the most popular concept in Probase is associated
%with more than 200 millions of bid keywords when we apply our approach
%to 702 millions of bid keywords.
%Traditional "inverted index" solution can't satisfy efficiency
%requirement in our case duing to the serious overhead of scanning
%posting lists of popular concepts.
%
%
%
%For the above analysis, we use some sophisticated schemes to select a
%set of candidate bid keywords for each query firstly.
%The scale of candidates set should be small enough so that computing
%semantic-matching scores for candidates within it will not consume
%intolerable time.
%Then we rank these candidates according to their semantic-matching
%score with given query.
%In order to ensure the quality of candidates, we make full use of
%click log to select candidates for popular query.
%The relevance between candidates and given query is estimated by their
%co-click information derived by mining the click log.
%For queries(or bid keywords) which do not have enough click
%information to draw credible conclusion, we select candidates for them
%using locality-sensitive hash(LSH) scheme, which can be regarded as a
%compromise between effectiveness(mainly recall) and efficiency.
\subsection{Click Data based Retrieval}
\label{sec:CSCD}
We use click through data to help find candidate bid keywords for a given
head query.  User clicks define strong associations between queries
and URLs~\cite{fuxman:keywordgeneration}.  Candidate selection using
click data (CSCD) is motivated by the assumption that queries leading
to clicks on the same URLs are semantically relevant.  We obtain
co-click data $\{(q,q',u,f)\}$ where $f$ is the number of times both
$q$ and $q'$ lead users to $u$.
%We use the following rules to
%filter out some co-click data. 
%\begin{itemize}
%\item $f\geq$ a pre-set threshold. 
%\item $q$ and $q'$ share at least one common term
%\item Edit similarity between $q$ and $q'$ is larger than $0.1$.
%\end{itemize}
We then build inverted index for the co-click data.  Given a query, we look it up in the index.  Assume the query exists in the index. For each of its co-clicked queries, we check if it is a bid
keyword using {\it exact match} and select those which are bid
keywords as candidates for the given query. If the query does not exist in the
index (it is a tail query), we turn to our second candidates selection
approach.
%for {\it  smart match}.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%version sep%%%%%%%%%%%%%%%%%%%%%%%%5
%As we know, the volume distribution of Web search queries follows the
%power law\cite{spink:theirqueries}.
%So there is no sufficient user behavior data(e.g., click data, session
%        data, etc.) for torso and tail queries.
%Worse, torso and tail queries constitute the majority and most of
%existing approaches do not do a good job on these ones.
%Indeed, We fail to select candidates for many queries due to their
%lack of sufficient click data.
\subsection{Concept Set based Retrieval}
\label{sec:CSJS}
%No matter a query is a head query or a tail query, we can
%conceptualize it into unit vector where each column corresponds to a
%concept in Probase.
%By conceptualization, we can transform a short text into a vector
%representation where each component correspond to a concept in
%Probase.
%By regarding the concept vector as a set where one concept belongs to
%it if the concept's corresponding coordinate is non-zero, 
There is no sufficient user behavior data for tail queries. Hence, we
select candidates for them based on their conceptualization results.
Since each short text is associated with a set of related concepts,
      we make assumption that the more similar two short texts are,
      the more similar their concept sets are and do not distinguish a
      short text from its corresponding concept set from now on. Under
      this assumption, we adopt Jaccard similarity ($\text{JS}(p,q)=\frac{\vert{}p\cap{}q\vert{}}{\vert{}p\cup{}q\vert{}}$)
    and believe that bid keywords with $\text{JS}(p,q)\geq{}t=0.8$ are
    somehow relevant to the given query $q$ and thus 
    %between their concept sets and do not distinguish a short text
    %from its corresponding concept set in the rest of this paper now.
%decide whether a query and a bid phrase is related.
%We set a threshold $t=0.8$ 
%so that a bid phrases whose Jaccard
%similarity with respect to the given query is no less
%than $t$ 
qualify for being the candidates of it.


%In our approach, 
%Thus, we can formalize our task as: 
%\begin{definition}
%Given a collection of bid phrases $P$, a collection of queries $Q$, a similarity
%function ($\text{JS}(\cdot,\cdot)$), a threshold
%$t=0.8$, to find all pairs of short texts $\{(p,q)\vert p\in P,q\in Q,
%    \text{JS}(p,q)\geq t\}$ 
%\end{definition}
%A naive algorithm needs $O(\vert Q\vert\cdot\vert P\vert)$ comparisons (exactly computing Jaccard
%    similarity) where $\vert P\vert$  is of billion order in our
%setting. 
%A popular solution is to convert Jaccard similarity constraint ($\text{JS}(p,q)\geq t$) into an equivalent
%overlap constraint ($\text{O}(p,q)\geq\frac{t}{1+t}(\vert p\vert+\vert
%            q\vert)=\alpha$) and build inverted index mapping each concept $c$
%to a list of bid phrases that contain $c$. %in their corresponding concept sets. 
%%a bid phrase qualifies for being candidate of a given query if the
%%Jaccard Similarity between its concept set and the given query's
%%concept set is no less than a threshold $t=0.8$.
%%Thus, the task of selecting candidates for 30 million queries from
%%0.7 billion bid phrases is a similarity join
%%problem and we use PPJoin to find all pairs of short texts whose
%%similarities are no smaller than $t$~\cite{xiao:ppjoin}.
%% . is under the assumption that relevant search phrases are more
%% likely to share similar concept sets.  We measure similarity between
%% concept sets using Jaccard Similarity.  Jaccard Similarity between two
%% concept sets $X$ and $Y$ is defined as follow:
%%\begin{definition}
%%\label{def:js}
%%\begin{equation}
%%\textnormal{Jaccard
%%    Similarity}(X,Y)=\frac{\vert{}X\cap{}Y\vert{}}{\vert{}X\cup{}Y\vert{}}
%%\end{equation}
%%\end{definition}
%%Obviously, we can't afford to apply Jaccard Similarity between the
%%given query and every bid phrase.
%%In such case, one solution is to build inverted index mapping concepts
%%to bid phrases that contain them and apply efficient large scale
%%similarity join algorithm such as PPJoin~\cite{xiao:ppjoin}.
%%In detail, 
%%and maintain a inverted index
%%mapping a concept $c$ to a list of short texts that contain $c$.  
%We scan each query $q\in Q$,
%   probing the index using every concept of $q$, and obtain
%   a set of bid phrases; merging these bid phrases together gives us
%   their actual overlap with $q$, final results
%   can be extracted by removing bid phrases whose overlap with $q$ is
%   less than $t$.
%However, some concepts contain extremely long inverted lists
%which incur significant overhead for building and accessing them.
%Moreover, according to our experiment, these popular concepts can't be
%treated as ``stop words'' and thus have to be reserved. 
%For large scale efficient similarity join, we adopt 
%%By the filtering rules of PPJoin, the number of bid phrases that
%%need verification (actually computing Jaccard Similarity) is
%%significantly reduced.
%PPJoin~\cite{xiao:ppjoin} which proposed several filtering rules 
%allowing us to scan only part of a concept's inverted list, to prune a
%considerable number of bid phrases in the inverted lists before
%actually computing Jaccard similarity.  Thus, the computaional
%effort for exactly comparison is significantly reduced.

%Although we use this technique to expanded 30 million queries with
%0.7 billion bid phrases within an acceptable period of time, these exact similarity
%search approaches seem not efficient enough.
%In order to make large scale similarity search efficiently,

%Constructing online query expansion system means for a query, we must
%select its most related bid phrases from $P$ within tens of
%millisecond.  Besides of the scalability of $P$, the not small number
%of elements of a concept set is also a challenge.  
Given a query $q$, to efficiently select bid keywords
$\{p\in{}P\vert{}\text{JS}(p,q)\geq{}t\}$, we give up those exact algorithms that have
to merge hundreds of posting lists of inverted index and adopt
locality-sensitive hash (LSH) which is \emph{approximate} algorithm
for finding similar items.


    %under some distance.
%  However, optimal solution is not as attractive as quickly
% retrieving a set of candidates with high enough quality.  We can
% ensure the precise and recall to some extend from probability
% perspective by setting several parameters of LSH tables appropriately.

% Although there are a lot of materials~\cite{raja:massivedatasets} presenting details of LSH, we
% give a brief introduction here for readers' convenience.
%We represent concept sets by a 0-1 matrix where each column represents
%one concept set and each row corresponds to one concept of Probase.
%There is a $1$ in row $r$ and column $c$ when the concept of $r$ is
%one element of concept set of $c$.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%version sep%%%%%%%%%%%%%%%%%%%%%%%%%%%
%We represent a concept set by a 0-1 vector where each component
%corresponds to one concept of Probase. 
%There is a $1$ in row $r$ when the corresponding concept of row $r$ is
%one element of the concept set.
%When we apply a minhash function $f(\cdot)$ to the 0-1 vector of a
%concept set, it applies a permuation over the rows of it.
%The minhash value of any vector (concept set) is the subscript of the
%first row in the permuted order in which the value is $1$.
%The most important property of minhash function $f(\cdot)$ is that
%\begin{equation}
%\label{eqn:minhashjs}
%\Pr[f(p)=f(q)]=\text{JS}(p,q)
%\end{equation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%version sep%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\vspace{-15pt}
%\begin{proof}
%Classify the rows of matrix representation into three classes:
%$\alpha$: both $X$ and $Y$ contain the concept of this row; $\beta$:
%only one of $X$ and $Y$ contains the concept of this row; $\gamma$:
%the concept of this row isn't included by neither $X$ nor $Y$.
%
%Obviously, according to definition \ref{def:js} we have:
%\begin{equation}
%\label{eqn:js1}
%\textnormal{Jaccard Similarity}(X,Y)=\frac{\vert\alpha\vert}{\vert\beta\vert+\vert\alpha\vert}
%\end{equation}
%
%On the other hand, based on the definition of minhash function,
%   $Pr[f(X)=f(Y)]=$ the fraction of ramdon permutations that rank the first
%   row of class $\alpha$ ahead the first row of class $\beta$.
%Thus, we have:
%\begin{equation}
%\label{eqn:minhash}
%\Pr[f(X)=f(Y)]=\frac{\vert\alpha\vert}{\vert\beta\vert+\vert\alpha\vert}
%\end{equation}
%
%The Eq.\eqref{eqn:js1} and Eq.\eqref{eqn:minhash} ensure the validation
%of Eq.\eqref{eqn:minhashjs}
%\end{proof}
%As \cite{raja:massivedatasets} defined:
%\begin{definition}
%A family $\mathcal{F}$ of functions is $(d_1,d_2,p_1,p_2)-sensitive$
%if $\forall f\in\mathcal{F}$:\\
%$d(x,y)\leq d_1\rightarrow Pr[f(x)=f(y)]\geq p_1$\\
%$d(x,y)\geq d_2\rightarrow Pr[f(x)=f(y)]\leq p_2$
%\end{definition}
%%%%%%%%%%%%%%%%%%%%%version sep%%%%%%%%%%%%%%%%%%%%%%%
%A minhash function $f(\cdot)$ maps an input concept set to an integer. 
The minhash function family $\mathcal{F}$ consists of many minhash functions
$f(\cdot)$ each maps input concept set to an integer with different
permutation manipulating concept sets.
If we randomly choose one minhash function $f(\cdot)$ from
$\mathcal{F}$, it will satisfy the following equation:
\begin{equation}
\label{eqn:minhash}
\Pr[f(p)=f(q)]=\text{JS}(p,q)
\end{equation}
As you can see, the larger Jaccard similarity between a bid phrase $p$ and the
given query $q$, the more likely they are to collide (mapped to the same value).
%Since Jaccard Distance is defined as one minus Jaccard Similarity, we
%can use Jaccard Similarity's minhash function family as
%$(d_1,d_2,1-d_1,1-d_2)-sensitive$ LSH family for Jaccard Distance.
%When $d_1<d_2$ which implies $p_1=1-d_1>p_2=1-d_2$, it will be useful
%for similar sets are more likely to be mapped into the same buckets.
%%%%%%%%%%%%%%%%%%%version sep%%%%%%%%%%%%%%%%%%%%%%%%%
Furthermore, to achieve satisfactory precision and recall, we use the banding
technique~\cite{raja:massivedatasets} to ``amplify'' the gap between 
collision probabilities of similar pairs and dissimilar pairs.
Firstly, $n=b\times{}r$ minhash functions are uniformly sampled from $\mathcal{F}$ with
replacement. 
%$f_{i,j}\in\mathcal{F},i=1,\ldots,r\text{ and }j=1,\ldots,b$ 
Then we can divide them into $b$ bands each of $r$ functions 
$\text{Band}_{i}(\cdot)=(f_{i,1}(\cdot),\ldots,f_{i,r}(\cdot)),i=1,\ldots,b$.
$\text{Band}_{i}(p)=\text{Band}_{i}(q)\Leftrightarrow{}\forall{}j\in\{1,\ldots,r\}:f_{i,j}(p)=f_{i,j}(q)$.
The collision (sharing same LSH signature) between two concept sets is
defined to be:
%LSH signature as follow:
%\begin{definition}
%$Sig(\cdot)=(B_{1}(\cdot),\ldots,B_{b}(\cdot))$ where $B_{i}(\cdot)=(f_{i,1}(\cdot),\ldots,f_{i,r}(\cdot))$
%\end{definition}
%We also define the collision(with the same signature) of two sets:
\begin{definition}
\label{def:col}
%$Sig(q)=Sig(w)\Leftrightarrow{}\exists i\in
%1,\ldots,b, B_{i}(q)=B_{i}(w) \textnormal{ where }\forall i,
%    B_{i}(q)=B_{i}(w)\Leftrightarrow\forall j\in1,\ldots,r,
%    f_{i,j}(q)=f_{i,j}(w)$
$\text{Sig}(p)=\text{Sig}(q)\Leftrightarrow{}\exists{}i\in\{1,\ldots,b\}$
s.t. $\text{Band}_{i}(p)=\text{Band}_{i}(q)$
\end{definition}
Suppose $\text{JS}(p,q)=d$, according to Definition \ref{def:col}, the probability of their
collision is $1-(1-d^r)^b$ which is a sharp S-curve (see Figure
        \ref{fig:scurve} where $r=8, b=16$). 
%Since we believe that
%bid phrases whose corresponding concept sets achieve no less than 
%$0.8$ Jaccard Similarity with respect to given query's concept set are
%suitable to be candidates, 
%%%%%%%%%%%%%%%%%%%version sep%%%%%%%%%%%%%%%%%%%%
%We should set $r$, $b$ appropriate values such that %the collision probabilities of true
%%candidates (JS$(b,q)\geq t$) near $1$.
%$\Pr[\text{Sig}(p)=\text{Sig}(q)]$ near $1$ for $(p,q)$ that satisfies
%$\text{JS}(p,q)\geq t$ while those not so similar bid phrases have low
%collision probabilities because recalled many false candidates will increase the
%computational effort (exactly computing the similarity) which means
%depression of efficiency.
%%%%%%%%%%%%%%%%%%%version sep%%%%%%%%%%%%%%%%%%%%
%Considering that those with less than $0.5$ Jaccard Similarity are so
%irrelevant,
%Based on above consideration, we choose parameters $r=8$, $b=16$ for
%our LSH tables.
\begin{figure}
\centering
%\epsfig{file=figures/scurve.eps, width=1.5in, height=1.3in}
\includegraphics[width=1.5in, height=1.3in]{figures/scurve.eps}
\caption{$\Pr[\textnormal{Sig}(p)=\textnormal{Sig}(q)]=1-(1-d^r)^b$}
\label{fig:scurve}
\end{figure}
%The curve that Jaccard Similarity of two concept sets versus
%probability that their LSH signatures collide is illustrated in Figure
%\ref{fig:scurve}.



%We pre-compute LSH signatures for bid phrases' corresponding
%concept sets and build the LSH tables.
We pre-computed LSH signatures for all $p\in{}P$.%bid phrases' corresponding concept
%sets.
For each $i\in\{1,\ldots,b\}$, we firstly aggregate bid
phrases $P$ over $\text{Band}_{i}(p)$.
Then we maintain the index by a key-value pair memory object caching
system where the value of $\text{Band}_{i}(p)$ are used as keys and lists of
aggregated bid phrases are used as values.
We maintain the index of each band in one specific machine.
%inverted index by which, given the LSH signature
%of the given query's concept set, we can efficie-ntly retrieve bid
%phrases whose corresponding concept sets are mapped into the same
%buckets.
At runtime, when a query $q$ is posed, we compute its concept set's LSH
signature and distribute $\text{Band}_{i}(q),i=1,\ldots,b$ to corresponding machine
respectively.
%and use the signature to retrieve bid phrases through our hash tables.
For each band, we check whether $\text{Band}_{i}(q)$ happens to be
the key of a certain key-value pair.
If it is the case, value (i.e., list of bid phrases) of the pair is
considered because these bid phrases satisfies
$\text{Sig}(p)=\text{Sig}(p)$ and thus have large Jaccard similarity with
the $q$ under some probabilistic guarantee.
Bid phrases in that list are calculated exact Jaccard similarity with
respect to $q$ and we only return at most 100 true positive ones as
well as their Jaccard similarity value.
Finally, we merge results returned from the $b$ machines
and rank these bid phrases by their $\text{JS}(p,q)$ in descending
order.
The top 100 bid phrases (if we recalled enough) are selected as
candidates for the given $q$.


%True positive ones are selected as candidates.
%The disjunctive relationship between different bands enables us to
%process each of them seperately.
By the LSH scheme, we replace cumbersome calculation such as merging
hundreds of posting lists of inverted index by %several access to only
%a few hash tables.
accessing $b$ hash tables.
Besides, the scalability of our approach partly relies on the banding
technique because the disjunctive relationship between different bands enables
us to maintain each band in one machine and process in a parallel way.
It is this feature that improves the scalability and efficiency of our
system.
%Since these selected candidates are unordered.
%We rank them by computing their exact Jaccard Similarity with respect
%to query's concept set and rank them in descending order and only reserve
%at most about one hundred of them.
%The connection between short text and concepts can be modeled as a
%bipartite graph and organized in adjacent lists which can be stored in
%distributed key-value pair in-memory storage system(e.g., memcached)
%    easily when the scale of corpus is too large to be held in memory
%    of one machine.
\subsection{Ranking}
%In fact, we can simply rank candidates by Jaccard similarity between
%their concept sets and the given query's.  Since Probase is a
%probabilistic taxonomy, measuring similarity between concept sets only
%exploits its rich isA relationships but hasn't made full use of its
%typicalities.
We assign a semantic-matching score (SMS) to each candidate bid phrase
with respect to a given query.  Then we rank these candidates by their
assigned SMS score in descending order and select top ranking bid
phrases.  Intuitively, SMS should be proportional to the semantic
similarity between short texts.  Since each short text is
characterized by a concept vector, we can estimate the similarity
between short texts by measuring the similarity between their
corresponding concept vectors.  Formally, for a given query $q$ and a
bid phrase $p$, with their corresponding concept vectors denoted as
$\mathbf{c_q}$ and $\mathbf{c_p}$, SMS is defined as follows:
\begin{definition}
\begin{equation}
\textnormal{SMS}(q,p)=\cos(\mathbf{c_q},\mathbf{c_p})=\mathbf{c_q}\cdot\mathbf{c_p}
\end{equation}
\end{definition}
%By filtering a large portion of bid keywords and only reserving a
%handful of high-quality candidatas which seem to be relevant to given
%query, we can compute SMS for candidates with respect to given query
%in a meticulous way while satisfing the efficiency requirement of
%online system.\\
%
%
%
%Firstly, we distinguish head instances from tail instances to
%re-weight concepts derived from these observed instances.
%In contrast of classic term weighting scheme such as TF-IDF which
%measures significance of a word to a certain document based on the
%frequence of the word within the content of the document as well as
%the frequency of the word in the whole corpus, we determine the
%significance of each instance according to its functionality with
%respect to expressing the meaning of the short text.
%For example, short text ``angry birds for windows phone 7'' refers to
%the ``wp7'' version of a popular game named ``angry birds''. Here
%``angry birds'' is a head instance but ``windows phone 7'' is a modifier
%instance.
%By semantically re-weight associated concepts of each short text, we
%can target the core meaning of short text more accurately.
%We present our work on distinguishing head/modifier in another
%paper.
%
%
%
%Since taxonomy used in our approach organizes concepts hierarchically
%by ``is-A'' relation, regarding each concept as a independent
%coordinate and adopting conventional metrics(e.g., cosine) to measure
%similarity between two concept vectors seems unreasonable.
%Search users with identical search intent but different extent of
%expertise may issue different queries.
%For example, customer has no idea about ``kindle'' may issue query
%``amazon ebook reader'' and ``kindle'' is more specific but still so
%relevant to his/her query.
%In order to exploit more specific or more general bid phrases to
%answer given query, we measure similarity between short texts with
%consideration of not only their corresponding concept vector but also both their parent concepts and children concepts.
%Given a concept vector $\mathbf{c}$, we denote its corresponding
%concept set as
%$C_{\mathbf{x}}=\{(c_1,w_1),(c_2,w_2),\ldots,(c_n,w_n)\}$ where
%$\forall i\in 1,\ldots,n,w_i>0$.
%We derive parents of $C_{\mathbf{x}}$ by regarding concepts within it
%as entities and ranking their hypernyms by equation
%\ref{eqn:naivebayesian}.
%Children of $C_{\mathbf{x}}$ is derived by ranking hyponyms of its
%concepts by:
%\begin{equation}
%P(e\vert C_{\mathbf{x}})=\frac{P(e)P(C_{\mathbf{x}}\vert
%        e)}{P(C_{\mathbf{x}})}\propto P(e)\prod_{i}^{n}P(c_i\vert e)
%\end{equation}
%Then we transform parents and children of $C_{\mathbf{x}}$ into
%concept vector form and normalize them resulting
%$\mathbf{x}_{\textnormal{p}},\mathbf{x},\mathbf{x}_{\textnormal{c}}$.
%Given two concept vectors $\mathbf{x}$ and $\mathbf{y}$, we define the
%SMS of them as:
%\begin{definition}
%SMS($\mathbf{x},\mathbf{y}$)=$\max_{t\in\{\mathbf{x}_{\textnormal{p}},\mathbf{x},\mathbf{x}_{\textnormal{c}}\}\times\{\mathbf{y}_{\textnormal{p}},\mathbf{y},\mathbf{y}_{\textnormal{c}}\}}\cos(t)$
%\end{definition}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "adselection"
%%% End:
